{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load in \n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the \"../input/\" directory.\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fastai==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import fastai\n",
    "\n",
    "from sklearn import metrics\n",
    "# from fastai.imports import *\n",
    "# from fastai.structured import *\n",
    "\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor , RandomForestClassifier\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "import io\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "sns.set()\n",
    "\n",
    "# train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./dataset/train.csv')\n",
    "test = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "train['date'] = pd.to_datetime(train['date'],format='%m/%d/%Y')\n",
    "# train['month'] = train['date'].dt.month\n",
    "train['dayofweek'] = train['date'].dt.dayofweek\n",
    "train['year'] = train['date'].dt.year\n",
    "train['day'] = train['date'].dt.day\n",
    "# train['dayofyear'] = train['date'].dt.dayofyear\n",
    "# train['weekofyear'] = train['date'].dt.weekofyear\n",
    "\n",
    "train.drop(['date'], axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = sns.FacetGrid(train, col='game_result')\n",
    "# g.map(plt.hist, 'Elo', bins=20)\n",
    "\n",
    "# g = sns.FacetGrid(train, col='game_result')\n",
    "# g.map(plt.hist, 'opp_Elo', bins=20)\n",
    "\n",
    "g = sns.FacetGrid(train, col='game_result')\n",
    "g.map(plt.hist, 'total_crowd', bins=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in train.iterrows():\n",
    "    if(row['bet_ratio']<0.25 and row['game_result']==0):\n",
    "        train.drop(index,inplace=True)  \n",
    "    elif(row['bet_ratio']>0.75 and row['game_result']==1):\n",
    "        train.drop(index,inplace=True)  \n",
    "    elif(row['opp_Elo']<1260 and row['opp_Elo']>1730):\n",
    "        train.drop(index,inplace=True)  \n",
    "    elif(row['Elo']>0.75 and row['Elo']==1):\n",
    "        train.drop(index,inplace=True)  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['team_id'] = train['team_id'].str.extract('(\\\\d+)', expand=True)\n",
    "train['opp_team_id'] = train['opp_team_id'].str.extract('(\\\\d+)', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Elo'].fillna(train.groupby('team_id')['Elo'].transform('mean'), inplace=True)\n",
    "train['opp_Elo'].fillna(train.groupby('opp_team_id')['opp_Elo'].transform('mean'), inplace=True)\n",
    "train['win_equivalent'].fillna(train.groupby('season_end')['win_equivalent'].transform('mean'), inplace=True)\n",
    "\n",
    "train = train.dropna(axis = 0, how ='any')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['Id'], axis=1, inplace=True)\n",
    "train.drop(['game_seq'], axis=1, inplace=True)  \n",
    "# train.drop(['season_end'], axis=1, inplace=True)        \n",
    "train.drop(['season_game_seq'], axis=1, inplace=True) \n",
    "# train.drop(['playoff'], axis=1, inplace=True)\n",
    "train.drop(['team_id'], axis=1, inplace=True) \n",
    "# train.drop(['Elo'], axis=1, inplace=True)\n",
    "train.drop(['opp_team_id'], axis=1, inplace=True)\n",
    "# train.drop(['opp_Elo'], axis=1, inplace=True) \n",
    "# train.drop(['win_equivalent'], axis=1, inplace=True) \n",
    "# train.drop(['bet_ratio'], axis=1, inplace=True) \n",
    "train.drop(['home_crowd'], axis=1, inplace=True) \n",
    "train.drop(['opp_crowd'], axis=1, inplace=True) \n",
    "train.drop(['total_crowd'], axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "train['playoff'] = labelencoder.fit_transform(train['playoff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[\"game_result\"]\n",
    "X = train.drop([\"game_result\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train , x_test, y_train, y_test = train_test_split(X,y,test_size=0.00005,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler=StandardScaler()\n",
    "# X_train = scaler.fit_transform(x_train)\n",
    "# X_test = scaler.transform(x_test)\n",
    "X_train = x_train\n",
    "X_test = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# n_estimators = [100, 300, 500, 800, 1200]\n",
    "# max_depth = [5, 8, 15, 25, 30]\n",
    "# min_samples_split = [2, 5, 10, 15, 100]\n",
    "# min_samples_leaf = [1, 2, 5, 10] \n",
    "\n",
    "# hyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n",
    "#               min_samples_split = min_samples_split, \n",
    "#              min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "# gridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, \n",
    "#                       n_jobs = -1)\n",
    "# bestF = gridF.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, tree\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = xgboost.XGBClassifier(random_state = 1, max_depth = 15,n_estimators = 10, min_samples_split = 2, min_samples_leaf = 1)\n",
    "model10=model1.fit(x_train, y_train)\n",
    "y_pred = model10.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error,r2_score\n",
    "y_pred= model1.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "lmse = mean_squared_log_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "lrmse = np.sqrt(lmse)\n",
    "print(\"R2 score:\", r2)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"LMSE:\", lmse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"LRMSE:\", lrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = svm.SVC()\n",
    "# model12=model2.fit(x_train, y_train)\n",
    "# y_pred = model12.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.metrics import mean_squared_log_error,r2_score\n",
    "# y_pred= model2.predict(X_test)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# lmse = mean_squared_log_error(y_test, y_pred)\n",
    "# rmse = np.sqrt(mse)\n",
    "# lrmse = np.sqrt(lmse)\n",
    "# print(\"R2 score:\", r2)\n",
    "# print(\"MAE:\", mae)\n",
    "# print(\"MSE:\", mse)\n",
    "# print(\"LMSE:\", lmse)\n",
    "# print(\"RMSE:\", rmse)\n",
    "# print(\"LRMSE:\", lrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forestOpt = RandomForestClassifier(random_state = 1, max_depth = 15,n_estimators = 50, min_samples_split = 2, min_samples_leaf = 1)\n",
    "                                   \n",
    "modelOpt = forestOpt.fit(x_train, y_train)\n",
    "y_pred = modelOpt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error,r2_score\n",
    "y_pred= forestOpt.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "lmse = mean_squared_log_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "lrmse = np.sqrt(lmse)\n",
    "print(\"R2 score:\", r2)\n",
    "print(\"MAE:\", mae)\n",
    "print(\"MSE:\", mse)\n",
    "print(\"LMSE:\", lmse)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"LRMSE:\", lrmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "test['date'] = pd.to_datetime(test['date'],format='%m/%d/%Y')\n",
    "# test['month'] = test['date'].dt.month\n",
    "test['dayofweek'] = test['date'].dt.dayofweek\n",
    "test['year'] = test['date'].dt.year\n",
    "test['day'] = test['date'].dt.day\n",
    "# test['dayofyear'] = test['date'].dt.dayofyear\n",
    "# test['weekofyear'] = test['date'].dt.weekofyear\n",
    "\n",
    "test.drop(['date'], axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['team_id'] = test['team_id'].str.extract('(\\\\d+)', expand=True)\n",
    "test['opp_team_id'] = test['opp_team_id'].str.extract('(\\\\d+)', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Elo'].fillna(test.groupby('team_id')['Elo'].transform('mean'), inplace=True)\n",
    "test['opp_Elo'].fillna(test.groupby('opp_team_id')['opp_Elo'].transform('mean'), inplace=True)\n",
    "test['win_equivalent'].fillna(test.groupby('season_end')['win_equivalent'].transform('mean'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Id = test['Id']\n",
    "test.drop(['Id'], axis=1, inplace=True)\n",
    "test.drop(['game_seq'], axis=1, inplace=True)  \n",
    "# test.drop(['season_end'], axis=1, inplace=True)        \n",
    "test.drop(['season_game_seq'], axis=1, inplace=True) \n",
    "# test.drop(['playoff'], axis=1, inplace=True)\n",
    "test.drop(['team_id'], axis=1, inplace=True) \n",
    "# test.drop(['Elo'], axis=1, inplace=True)\n",
    "test.drop(['opp_team_id'], axis=1, inplace=True)\n",
    "# test.drop(['opp_Elo'], axis=1, inplace=True) \n",
    "# test.drop(['win_equivalent'], axis=1, inplace=True) \n",
    "# test.drop(['bet_ratio'], axis=1, inplace=True) \n",
    "test.drop(['home_crowd'], axis=1, inplace=True) \n",
    "test.drop(['opp_crowd'], axis=1, inplace=True) \n",
    "test.drop(['total_crowd'], axis=1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "test['playoff'] = labelencoder.fit_transform(test['playoff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test=model1.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test=prediction_test.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_solution=pd.DataFrame(prediction_test,Id,columns=[\"game_result\"])\n",
    "print(my_solution.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_solution['game_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_solution.to_csv(r'my_solution1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'my_solution1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
